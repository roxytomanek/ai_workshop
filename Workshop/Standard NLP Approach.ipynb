{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Automatic Document Categorization <br> with Machine Learning techniques in Python </center>\n",
    "\n",
    "**Authors**: Adam Karwan, Roksana Tomanek, Aleksander Zajchowski i Sviatoslav Somov\n",
    "\n",
    "### Before we start\n",
    "\n",
    "- You can find all files and information in this Github Repository : https://github.com/roxytomanek/ai_workshop\n",
    "\n",
    "### What are we going to achieve\n",
    "\n",
    "**Can you use this dataset to build a prediction model that will accurately classify which texts are spam?**\n",
    "\n",
    "##### Context\n",
    "The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged acording being ham (legitimate) or spam.\n",
    "##### Content\n",
    "The files contain one message per line. Each line is composed by two columns: v1 contains the label (ham or spam) and v2 contains the raw text.\n",
    "This corpus has been collected from free or free for research sources at the Internet.\n",
    "##### Acknowledgements\n",
    "The original dataset can be found here: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Importinig libraries\n",
    "\n",
    "- **NumPy** - package for scientific computing; it provides support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays\n",
    "- **Pandas** - we use this library for data manipulation and analysis; it offers data structures and operations for manipulating numerical tables and time series\n",
    "- **Matplotlib** - is a plotting library for the Python programming language and a numerical mathematics extension NumPy. Also **Pyplot** is a Matplotlib module which provides a MATLAB-like interface but it's free and open-source\n",
    "- **Seaborn** - is a data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "- **Scikit-learn** (sklearn) is a free software machine learning library. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, it's also designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.\n",
    "- **Pickle** - The pickle module implements binary protocols for serializing and de-serializing a Python object structure. “Pickling” is the process where a Python object hierarchy is converted into a byte stream.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries \n",
    "import numpy as np\n",
    "import pandas as pd # Dataframe Management\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns # Visualization\n",
    "from sklearn.model_selection import train_test_split \n",
    "import pickle # Model Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Downloading the dataset\n",
    "\n",
    "The first step to any data science project is to import your data. Often, you'll work with data in Comma Separated Value (CSV) files and run into problems at the very beggining of your workflow. To load a csv file we're often using `read_csv()` function from `pandas`. In the round brackets, you can use arguments to adjust the process to your needs. In this case, we're using `delimiter` and `encoding`, all of the possibilities you can find in the documentation.\n",
    "\n",
    "If you want to see how the DataFrame looks like you can try using this commands:\n",
    "\n",
    "- `df.head(10)` - this command will show you the fist 10 rows of the DataFrame\n",
    "- `df.tail(10)` - this one will show you the last 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = pd.read_csv('./data/spam_or_ham.csv', delimiter=',', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Summarize the Dataset\n",
    "\n",
    "Now we need to look at the data following this 3 steps:\n",
    "\n",
    "1. Dimensions of the dataset.\n",
    "2. Statistical summary of all attributes.\n",
    "3. Class Distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Analysis\n",
    "# View Dataset, top 10 Text Messages\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensions of the dataset\n",
    "\n",
    "We can get a quick idea of how many instances (rows) and how many attributes (columns) the data contains with the shape property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistical summary\n",
    "\n",
    "Now we can take a look at a summary of each attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Distribution\n",
    "Now let's take a look at the number of instances (rows) that belong to each class. We can view this as an absolute count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class distribution\n",
    "print(df.groupby('Label').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Data Visualization\n",
    "\n",
    "After the explortory analysis we now have a basic idea about the data, but it's always easier to see it in a graph :) \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll see how the class distribution looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Distribution - Not Balanced Data\n",
    "sns.countplot(df.Label)\n",
    "plt.xlabel('Label')\n",
    "plt.title('Number of ham and spam messages')\n",
    "# 20% Spam Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check what words appear in each class. \n",
    "\n",
    "We'll use `WordCloud` library for this visualization. \n",
    "If you're environment doesn't have it preinstalled just use \n",
    "`conda install -c conda-forge wordcloud` in your terminal if Anaconda is installed \n",
    "or `pip install wordcloud`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud \n",
    "\n",
    "# spam and ham words\n",
    "spam_words = ' '.join(list(df[df['Label'] == 'spam']['Text']))\n",
    "ham_words = ' '.join(list(df[df['Label'] == 'ham']['Text']))\n",
    "\n",
    "# Create Word Clouds \n",
    "spam_wc = WordCloud(width = 512, height = 512, colormap = 'plasma').generate(spam_words)\n",
    "ham_wc = WordCloud(width = 512, height = 512, colormap = 'ocean').generate(ham_words)\n",
    "\n",
    "# Plot Word Clouds\n",
    "# SPAM\n",
    "plt.figure(figsize = (10,8), facecolor = 'r')\n",
    "plt.imshow(spam_wc)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()\n",
    "\n",
    "# HAM \n",
    "plt.figure(figsize = (10,8), facecolor = 'g')\n",
    "plt.imshow(ham_wc)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()\n",
    "\n",
    "# In Spam Messages word FREE occurs very oftenly\n",
    "# In Ham Messages words 'OK', 'will', 'got' occur often and corrupted words ('gt' or 'lt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Models evaluation\n",
    "\n",
    "1. Separate out a validation dataset\n",
    "2. Build models\n",
    "3. Select the best model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating train and test datasets\n",
    "\n",
    "To evaluate the model we're going to split it into two parts, one is for training and the second for testing. In this step we're using `train_test_split` function. It will be splited with the `test_size=0.3`, it means we will use 70% of data to train our models and 30% we will hold back as a validation dataset. `random_state` as the name suggests, is used for initializing the internal random number generator, which will decide the splitting of data into train and test indices in your case. Setting `random_state`, a fixed value, will guarantee that same sequence of random numbers will be generated each time you run the code. And unless there is some other randomness present in the process, the results produced will be same as always. This helps in verifying the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data Set into Train and Test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(df.Text, df.Label, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODELS\n",
    "\n",
    "Now the fun begins! In this step, we'll run three different models and see the results for each of them. Here is a simple task for you - in the cell below we definned these three models and your job is to rerun the cell with each of them. \n",
    "\n",
    "But before we'll do it - some brief models explanation:\n",
    "\n",
    "##### Naive Bayes\n",
    "\n",
    "It is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, all of these properties independently contribute to the probability that this fruit is an apple and that is why it is known as ‘Naive’.\n",
    "\n",
    "Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.\n",
    "More information and tutorial :  <br>\n",
    "https://machinelearningmastery.com/naive-bayes-tutorial-for-machine-learning/<br>\n",
    "https://towardsdatascience.com/introduction-to-naive-bayes-classification-4cffabb1ae54\n",
    "\n",
    "\n",
    "##### Support Vector Machine\n",
    "\n",
    "A support vector machine (SVM) is a type of supervised machine learning classification algorithm. SVMs were introduced initially in 1960s and were later refined in 1990s. However, it is only now that they are becoming extremely popular, owing to their ability to achieve brilliant results. SVMs are implemented in a unique way when compared to other machine learning algorithms.\n",
    "\n",
    "In case of linearly separable data in two dimensions, a typical machine learning algorithm tries to find a boundary that divides the data in such a way that the misclassification error can be minimized. But in fact, there can be several boundaries that correctly divide the data points. SVM differs from the other classification algorithms in the way that it chooses the decision boundary that maximizes the distance from the nearest data points of all the classes. An SVM doesn't merely find a decision boundary; it finds the most optimal decision boundary.\n",
    "You can read more about it here: <br>\n",
    "https://towardsdatascience.com/introduction-to-support-vector-machine-svm-4671e2cf3755\n",
    "\n",
    "##### Random Forest\n",
    "\n",
    "Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. A random forest is a classifier consisting of a collection of tree structured classifiers {h(x,Θk ), k=1, …} where the {Θk} are independent identically distributed random vectors and each tree casts a unit vote for the most popular class at input x. Briefly, Random forest builds multiple decision trees and merges them together to get a more accurate and stable prediction.\n",
    "\n",
    "Tutorial and more information:<br>\n",
    "https://towardsdatascience.com/random-forest-in-python-24d0893d51c0<br>\n",
    "https://machinelearningmastery.com/implement-random-forest-scratch-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instruction\n",
    "\n",
    "Now your task! In lines **18-20** inside a function are 3 models definned, and only one of them is active. To see results for each of them add or remove `#` in the beginning of the line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB # Naive Bayes\n",
    "from sklearn.svm import LinearSVC, SVC # Support Vector Machine\n",
    "from sklearn.ensemble import RandomForestClassifier # Random Forest\n",
    "import time\n",
    "\n",
    "# Python Function\n",
    "def models(list_sentences_train, list_sentences_test, train_labels, test_labels):\n",
    "    t0 = time.time() # start time\n",
    "    \n",
    "    # Pipeline \n",
    "    model = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))), \n",
    "                      ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                      ('clf', MultinomialNB())]) # Naive Bayes\n",
    "    #                  ('clf', SVC(kernel='linear', probability=True))]) # Linear SVM with probability\n",
    "    #                  ('clf', RandomForestClassifier())]) # Random Forest\n",
    "\n",
    "    # Train Model\n",
    "    model.fit(list_sentences_train, train_labels) \n",
    "    \n",
    "    duration = time.time() - t0 # end time\n",
    "    print(\"Training done in %.3fs \" % duration)\n",
    "\n",
    "    # Model Accuracy\n",
    "    print('Model final score: %.3f' % model.score(list_sentences_test, test_labels))\n",
    "    return model\n",
    "\n",
    "# Train, Evaluate and Save Model\n",
    "model_std_NLP = models(X_train, X_test, Y_train, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above we only have numerical output, so for making it easier to understand we'll visualise it and create **confusion matrix**\n",
    "\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known.\n",
    "What can we learn from this matrix?\n",
    "- There are two possible predicted classes: \"spam\" and \"ham\". In this case if we are predicting the purpose of a message, \"spam\" means that it's a spam message, and \"ham\" is for normal messages.\n",
    "\n",
    "<br>Let's now define the most basic terms:\n",
    "- true positives (TP): These are the cases in which we predicted \"ham\" (important message).\n",
    "- true negatives (TN): We found \"spam\".\n",
    "- false positives (FP): We predicted ham, but it's actually spam. (Also known as a \"Type I error.\")\n",
    "- false negatives (FN): We predicted spam, but they are actually imporant messages. (Also known as a \"Type II error.\")\n",
    "<br>\n",
    "\n",
    "\n",
    "Very useful article: https://towardsdatascience.com/understanding-confusion-matrix-a9ad42dcfd62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\n",
    "from sklearn.metrics import confusion_matrix # Library to Compute Confusion Matrix\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    # classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Predictions with model\n",
    "Y_pred = model_std_NLP.predict(X_test)\n",
    "class_names = np.array(['ham', 'spam'])\n",
    "\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(Y_test, Y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(Y_test, Y_pred, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle\n",
    "\n",
    "Pickle is used for serializing and de-serializing Python object structures, also called marshalling or flattening. Serialization refers to the process of converting an object in memory to a byte stream that can be stored on disk or sent over a network. Later on, this character stream can then be retrieved and de-serialized back to a Python object. Pickling is not to be confused with compression! The former is the conversion of an object from one representation (data in Random Access Memory (RAM)) to another (text on disk), while the latter is the process of encoding data with fewer bits, in order to save disk space.\n",
    "\n",
    "Pickling is useful for applications where you need some degree of persistency in your data. Your program's state data can be saved to disk, so you can continue working on it later on. It can also be used to send data over a Transmission Control Protocol (TCP) or socket connection, or to store python objects in a database. Pickle is very useful for when you're working with machine learning algorithms, where you want to save them to be able to make new predictions at a later time, without having to rewrite everything or train the model all over again.\n",
    "\n",
    "Useful: https://www.pythoncentral.io/how-to-pickle-unpickle-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file in the current working directory\n",
    "pkl_filename = \"pickle_model.pkl\"  \n",
    "with open(pkl_filename, 'wb') as file:  \n",
    "    pickle.dump(model_std_NLP, file)\n",
    "\n",
    "# Load from file\n",
    "with open(pkl_filename, 'rb') as file:  \n",
    "    pickle_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_spam = ['Urgent! call 09066350750 from your landline. Your complimentary 4* Ibiza Holiday or 10,000 cash await collection SAE T&Cs PO BOX 434 SK3 8WP 150 ppm 18+ ']\n",
    "test_text_ham = ['Good. No swimsuit allowed :)']\n",
    "\n",
    "# Predict Category and Probability\n",
    "# Spam\n",
    "print(model_std_NLP.predict(test_text_spam)) \n",
    "print(model_std_NLP.predict_proba(test_text_spam)) \n",
    "\n",
    "# Ham\n",
    "print(model_std_NLP.predict(test_text_ham)) \n",
    "print(model_std_NLP.predict_proba(test_text_ham)) \n",
    "\n",
    "# More Test Examples\n",
    "# Ham - 0\n",
    "# Good. No swimsuit allowed :)\n",
    "# Wish i were with you now!\n",
    "# Im sorry bout last nite it wasnÃ¥Ãt ur fault it was me, spouse it was pmt or sumthin! U 4give me? I think u shldxxxx\n",
    "\n",
    "# Spam - 1\n",
    "# Urgent! call 09066350750 from your landline. Your complimentary 4* Ibiza Holiday or 10,000 cash await collection SAE T&Cs PO BOX 434 SK3 8WP 150 ppm 18+ \n",
    "# +123 Congratulations - in this week's competition draw u have won the Ã¥Â£1450 prize to claim just call 09050002311 b4280703. T&Cs/stop SMS 08718727868. Over 18 only 150ppm\n",
    "# Double mins and txts 4 6months FREE Bluetooth on Orange. Available on Sony, Nokia Motorola phones. Call MobileUpd8 on 08000839402 or call2optout/N9DX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Pickle Model\n",
    "print(pickle_model.predict(test_text_spam)) # Predict Category\n",
    "print(pickle_model.predict_proba(test_text_spam)) # Predict Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
